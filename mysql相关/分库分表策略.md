#### 简介

随着业务数据的递增数据库就会产生瓶颈，出现资源报警、查询缓慢等现象

单机数据库所能承受的连接数、I/O及网络的吞吐量等都是有限的，所以当并发量上来之后，数据库就渐渐支撑不了我们的的业务需要了，加之单表的数据库过大，查询的性能也会下降，因此分库分表势在必行

分表是为了解决单表数据量太大而导致的慢查询

分库是为了解决服务器资源受单机限制，处理不了高并发的问题，需要把部分请求分配到其他服务器上，以降低服务器的压力


一般按照业务进行分库，例如活动是业务的主流程，访问量非常大

分库会引发的问题：

1. 不能再使用单机的事务处理，需要使用分布式事务来解决

2. 不支持 JOIN 连接查询，需要使用程序代替分步查询，或表里冗余字段，如： user_name


不管是分库还是分表，我们一般都有两种方式应对，一种是垂直拆分，另一种是水平拆分

#### 垂直分表

垂直分表就是把一些不常用的大字段剥离出去，一个数据页的空间是有限的，把一些无用的数据拆分出去，一页就能存放更多行的数据

例如：

活动表中有活动标题、主图、价格、开始时间、活动详情、点击量、收藏量，一个活动列表通常会有标题、主图、价格，其他字段并不是常用到的，特别是活动详情，可以把这些比较少用的字段分离到附表中

注意：

垂直分表需要关联字段

#### 水平分表

水平分表是因为一张表内的数据太多了，访问的性能就差，所以进行水平拆分

水平分表因数据量大而拆分成多个节点，就会存在以下问题：

> 排序、统计记录数 count 、分页问题

使用业务代码来实现，或使用中间件将各个表中的数据汇总、排序、分页然后返回，对于查询条件不变的情况可以缓存 count ，通过增删更新计数

> 路由问题

- Hash 路由

通过 Key 进行 Hash 运算，将 Hash 运算得到的结果再对节点数进行取模，这样就能均匀的将数据分到不同的节点上，数据分布均匀，但后期增加节点会增加难度，需要翻倍扩容法和一致性 Hash 扩容法

- 范围路由

表示一定范围的数据，地区范围、时间段等，容易扩展但数据分布不一定均匀

- 路由表

专门设计一个表来记录路由信息，记录 Key 映射到某个表，每次查询都需要访问路由表（通常这里可以放到缓存），但迁移数据时比较灵活，在路由表中修改映射关系即可

> 全局主键问题

不再是单表的主键自增，需要一些策略保证全局主键唯一

1. 依然是主键自增，只是初始值不一样，自增步长设置一样就不会重复

2. UUID，但是不连续的主键插入会导致严重的页分裂，性能比较差

3. 分布式 ID，比较出名的就是 Twitter 开源的 sonwflake 雪花算法，或者使用单线程的 redis 原子操作 INCR和INCRBY来实现

对于订单数据，可以使用 buyer_uid 作为分表索引，也可以使用 seller_uid 作为分表索引，无论采用哪种都不能完全满足所有的查询需求，一般需要冗余数据，例如可以将订单数据异步同步到一张专门的表中提供给商家使用，或将数据异步同步到 ES 中

#### 常见扩容方案

> 翻倍扩容法

翻倍扩容法的主要思维是每次扩容，库的数量均翻倍处理，而翻倍的数据源通常是由原数据源通过主从复制方式得到的，是一种从库升级成主库后提供服务的方式。故有些文档将其称作"从库升级法"

1. 为每个节点都新增从库，开启主从同步进行数据同步
2. 主从同步完成后，对主库进行禁写，断开主从关系
3. 从库升级为集群节点，业务应用识别到新的分库数后，将应用新的路由算法
4. 确定所有的应用均接受到库总数的配置后，放开原主库的禁写操作，此时应用完全恢复服务
5. 可选性删除冗余数据，例如： delete from db1.tbl0 where hash_val mod 2 <> 1;


翻倍扩容法有停止写的服务，且每次扩容均需要对库数量进行翻倍，会提前浪费不少的数据库资源

> 一致性哈希扩容法

1. 针对需要扩容的数据库节点增加从节点，开启主从同步进行数据同步
2. 完成主从同步后，对原主库进行禁写，断开主从关系
3. 修改一致性 Hash 范围的配置，并使应用服务重新读取并生效
4. 确定所有的应用均接受到新的一致性 Hash 范围配置后，放开原主库的禁写操作，此时应用完全恢复服务
5. 可选性删除冗余数据，例如： delete from db1.tbl0 where hash_val mod 2 <> 1;

一致性哈希扩容法和翻倍扩容法的方案比较类似，但是它更加灵活，可以根据当前集群每个节点的压力情况选择性扩容，而无需整个集群同时翻倍进行扩容


举个例子，我们使用最简单的 Hash 算法 Table = hash(Key) mod N 来实现分表，不难发现，这样的 Hash 只要是节点数量 N 发生了变化，之前的所有 Hash 映射就会全部失效

一致性 Hash 通过构建环状的 Hash 空间代替线性 Hash 空间的方法解决这个问题，使用一致性 Hash 时需要进行两次映射：

1. 给每个节点计算 Hash ，然后记录它们的 Hash 值，这就是它们在环上的位置
2. 给每个 Key 计算 Hash，然后沿着顺时针的方向找到环上的第一个节点，就是该 Key 储存对应的节点


增加或删除一个节点，其他节点所在环上的映射不会发生变化，只是原来打在对应节点上的 Key 现在会转移到顺时针方向的下一个节点上去，但还是有少部分的 Key 发生了失效，且整体系统的压力也已经不是均衡的了

如果节点的数量很少，而 Hash 环空间很大（一般是 0 ~ 2^32），直接进行一致性 Hash 上去，大部分情况下节点在环上的位置会很不均匀，挤在某个很小的区域，最终导致每个节点的数据量不一致，会发生严重的数据倾斜

Table = hash(Key) mod 2^32

我们可以通过扩展整个环上的节点数量引入虚拟节点的概念。一个实际节点将会映射多个虚拟节点，这样 Hash 环上的空间分割就会变得均匀，引入虚拟节点还会使得节点在 Hash 环上的顺序随机化，这意味着当一个真实节点失效退出后，它原来所承载的压力将会均匀地分散到其他节点上去。比如 RedisCluster 即是通过一致性 Hash 算法，使用 16384 个虚拟槽节点进行每个分片数据的管理

最后我们就可以根据实际的节点数量添加相应的虚拟节点构成一个环状的均衡服务
